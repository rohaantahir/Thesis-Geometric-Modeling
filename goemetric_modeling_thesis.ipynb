{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "goemetric-modeling-thesis",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w9EmodNdgM0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "a5e18ec4-fa7e-4ef9-8488-c2f97278e171"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/2d/c8e052ba51099faee0bfe71d84f35bb1576e6910483cad46b840a122ca6c/PyDrive-1.3.1-py2-none-any.whl\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python2.7/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python2.7/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python2.7/dist-packages (from pydrive) (1.7.9)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0->pydrive) (0.11.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.5)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0->pydrive) (1.15.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client>=1.2->pydrive) (0.0.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client>=1.2->pydrive) (1.17.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (44.1.1)\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0BXmOvGhcIy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a1927d4-8372-4ce6-869e-82c7a32c0384"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsNlBylIuSdC"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHgkmmw0eHny",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "74c73c67-df91-4b6a-ebca-20fd5aaa7776"
      },
      "source": [
        "!pip install tensorflow-gpu==1.13.1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==1.13.1 in /usr/local/lib/python2.7/dist-packages (1.13.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (3.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (0.35.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (0.7.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (1.0.post1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow-gpu==1.13.1) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.13.1) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.13.1) (5.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (44.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUk6eYJToI-1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2a4b2a4-044e-4e1a-91ca-5ed06b0a0172"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4kOqyLAfC_M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74ca0597-9208-4f6d-c6d0-857398dd335f"
      },
      "source": [
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default GPU Device: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8IaA34Jg6iB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f30e9ca-a79b-4c5d-f778-22500f8a4af7"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iOnK98bNQbj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-gLAe_dONP2"
      },
      "source": [
        "def append_new_line(file_name, text_to_append):\n",
        "    \"\"\"Append given text as a new line at the end of file\"\"\"\n",
        "    # Open the file in append & read mode ('a+')\n",
        "    with open(file_name, \"a+\") as file_object:\n",
        "        # Move read cursor to the start of file.\n",
        "        file_object.seek(0)\n",
        "        # If file is not empty then append '\\n'\n",
        "        data = file_object.read(100)\n",
        "        if len(data) > 0:\n",
        "            file_object.write(\"\\n\")\n",
        "        # Append text at the end of file\n",
        "        file_object.write(text_to_append)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3phwagYjPIXi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0VBh9o5PhzI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "magLgj4SrPLl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "8aaa54ff-ae55-46cd-c302-c6e58180799b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/im2avatar/utils/')\n",
        "sys.path.append('/content/drive/My Drive/im2avatar/models/')\n",
        "\n",
        "import dataset as dataset\n",
        "import model_shape as model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-31b67a06e2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/im2avatar/models/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_shape\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: No module named dataset",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBNXui5zejcH"
      },
      "source": [
        "\n",
        "#FLAGS = tf.app.flags.FLAGS\n",
        "#tf.app.flags.DEFINE_string('train_dir', './content/drive/My Drive/im2avatar/train_shape',\n",
        "#                           \"\"\"Directory where to write summaries and checkpoint.\"\"\")\n",
        "#tf.app.flags.DEFINE_string('base_dir', './content/drive/My Drive/im2avatar/data/ShapeNetCore_im2avatar', \n",
        "#                           \"\"\"The path containing all the samples.\"\"\")\n",
        "#tf.app.flags.DEFINE_string('cat_id', '02958343', \n",
        "#                           \"\"\"The category id for each category: 02958343, 03001627, 03467517, 04379243\"\"\")\n",
        "#tf.app.flags.DEFINE_string('data_list_path', './content/drive/My Drive/im2avatar/data_list', \n",
        "#                          \"\"\"The path containing data lists.\"\"\")\n",
        "\n",
        "#tf.app.flags.DEFINE_integer('train_epochs', 501, \"\"\"Training epochs.\"\"\")\n",
        "#tf.app.flags.DEFINE_integer('batch_size', 60, \"\"\"Batch size.\"\"\")\n",
        "#tf.app.flags.DEFINE_integer('gpu', 0, \"\"\"\"\"\")\n",
        "#tf.app.flags.DEFINE_float('learning_rate', 0.0003, \"\"\"\"\"\")\n",
        "#tf.app.flags.DEFINE_float('wd', 0.00001, \"\"\"\"\"\")\n",
        "#tf.app.flags.DEFINE_integer('epochs_to_save',20, \"\"\"\"\"\")\n",
        "#tf.app.flags.DEFINE_integer('decay_step',20000, \"\"\"for lr\"\"\")\n",
        "#tf.app.flags.DEFINE_float('decay_rate', 0.7, \"\"\"for lr\"\"\")\n",
        "\n",
        "IM_DIM = 128 \n",
        "VOL_DIM = 64 \n",
        "\n",
        "BATCH_SIZE = 60\n",
        "TRAIN_EPOCHS = 301\n",
        "GPU_INDEX = 0\n",
        "BASE_LEARNING_RATE = 0.0003\n",
        "wd = 0.00001\n",
        "DECAY_STEP = 20000\n",
        "DECAY_RATE = 0.7\n",
        "cat_id = \"03467517\"\n",
        "\n",
        "BN_INIT_DECAY = 0.5\n",
        "BN_DECAY_DECAY_RATE = 0.5\n",
        "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
        "BN_DECAY_CLIP = 0.99\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU_INDEX)\n",
        "\n",
        "TRAIN_DIR = os.path.join(\"/content/drive/My Drive/im2avatar/train_shape\", cat_id)\n",
        "if not os.path.exists(TRAIN_DIR): \n",
        "  os.makedirs(TRAIN_DIR)\n",
        "LOG_FOUT = open(os.path.join(TRAIN_DIR, 'log_train.txt'), 'w')\n",
        "#LOG_FOUT.write(str(tf.flags._global_parser.parse_args())+'\\n')\n",
        "\n",
        "def log_string(out_str):\n",
        "  LOG_FOUT.write(out_str+'\\n')\n",
        "  LOG_FOUT.flush()\n",
        "  print(out_str)\n",
        "\n",
        "\n",
        "def get_learning_rate(batch):\n",
        "  learning_rate = tf.train.exponential_decay(\n",
        "                      BASE_LEARNING_RATE,  # Base learning rate.\n",
        "                      batch * BATCH_SIZE,  # Current index into the dataset.\n",
        "                      DECAY_STEP,          # Decay step.\n",
        "                      DECAY_RATE,          # Decay rate.\n",
        "                      staircase=True)\n",
        "  learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
        "  return learning_rate \n",
        "\n",
        "\n",
        "def get_bn_decay(batch):\n",
        "  bn_momentum = tf.train.exponential_decay(\n",
        "                    BN_INIT_DECAY,\n",
        "                    batch*BATCH_SIZE,\n",
        "                    BN_DECAY_DECAY_STEP,\n",
        "                    BN_DECAY_DECAY_RATE,\n",
        "                    staircase=True)\n",
        "  bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
        "  return bn_decay \n",
        "\n",
        "\n",
        "def train(dataset_):\n",
        "  with tf.Graph().as_default():\n",
        "    with tf.device('/gpu:'+str(GPU_INDEX)):\n",
        "      tf.test.gpu_device_name()\n",
        "      is_train_pl = tf.placeholder(tf.bool)\n",
        "      img_pl, vol_pl = model.placeholder_inputs(BATCH_SIZE, IM_DIM, VOL_DIM)\n",
        "\n",
        "      global_step = tf.Variable(0)\n",
        "      bn_decay = get_bn_decay(global_step)\n",
        "      tf.summary.scalar('bn_decay', bn_decay)\n",
        "\n",
        "      pred = model.get_model(img_pl, is_train_pl, weight_decay=wd, bn_decay=bn_decay)\n",
        "      loss = model.get_MSFE_cross_entropy_loss(pred, vol_pl)\n",
        "      tf.summary.scalar('loss', loss)\n",
        "\n",
        "      learning_rate = get_learning_rate(global_step)\n",
        "      tf.summary.scalar('learning_rate', learning_rate)\n",
        "      optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "      train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "      summary_op = tf.summary.merge_all()\n",
        "\n",
        "      saver = tf.train.Saver()\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allocator_type = 'BFC'\n",
        "    config.gpu_options.allow_growth = True\n",
        "    config.allow_soft_placement = True\n",
        "\n",
        "    with tf.Session(config=config) as sess:\n",
        "      model_path = os.path.join(TRAIN_DIR, \"trained_models\")\n",
        "      if tf.gfile.Exists(os.path.join(model_path, \"checkpoint\")):\n",
        "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
        "        restorer = tf.train.Saver()\n",
        "        restorer.restore(sess, ckpt.model_checkpoint_path)\n",
        "        print (\"Load parameters from checkpoint.\")\n",
        "      else:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "      train_summary_writer = tf.summary.FileWriter(model_path, graph=sess.graph)\n",
        "\n",
        "      train_sample_size = dataset_.getTrainSampleSize()\n",
        "      train_batches = train_sample_size // BATCH_SIZE # The number of batches per epoch\n",
        "\n",
        "      val_sample_size = dataset_.getValSampleSize()\n",
        "      val_batches = val_sample_size // BATCH_SIZE\n",
        "\n",
        "      for epoch in range(TRAIN_EPOCHS):\n",
        "        ####################\n",
        "        # For training\n",
        "        ####################\n",
        "        dataset_.shuffleIds()\n",
        "        for batch_idx in range(train_batches):\n",
        "          imgs, vols_clr = dataset_.next_batch(batch_idx * BATCH_SIZE, BATCH_SIZE, vol_dim=VOL_DIM)\n",
        "          vols_occu = np.prod(vols_clr > -0.5, axis=-1, keepdims=True) # (batch, vol_dim, vol_dim, vol_dim, 1)\n",
        "          vols_occu = vols_occu.astype(np.float32)\n",
        "\n",
        "          feed_dict = {img_pl: imgs, vol_pl: vols_occu, is_train_pl: True}\n",
        "\n",
        "          step = sess.run(global_step)\n",
        "          _, loss_val = sess.run([train_op, loss], feed_dict=feed_dict)\n",
        "\n",
        "          log_string(\"<TRAIN> Epoch {} - Batch {}: loss: {}.\".format(epoch, batch_idx, loss_val))\n",
        "          append_new_line('/content/drive/My Drive/im2avatar/logs of training/log.txt', \"<TRAIN> Epoch {} - Batch {}: loss: {}.\".format(epoch, batch_idx, loss_val))\n",
        "    \n",
        "        #####################\n",
        "        # For validation\n",
        "        #####################\n",
        "        loss_sum = 0.0\n",
        "        for batch_idx in range(val_batches):\n",
        "          imgs, vols_clr = dataset_.next_batch(batch_idx * BATCH_SIZE, BATCH_SIZE, vol_dim=VOL_DIM,  process=\"val\")\n",
        "          vols_occu = np.prod(vols_clr > -0.5, axis=-1, keepdims=True) # (batch, vol_dim, vol_dim, vol_dim, 1)\n",
        "          vols_occu = vols_occu.astype(np.float32)\n",
        "\n",
        "          feed_dict = {img_pl: imgs, vol_pl: vols_occu, is_train_pl: False}\n",
        "          \n",
        "          loss_val = sess.run(loss, feed_dict=feed_dict)\n",
        "          loss_sum += loss_val\n",
        "        log_string(\"<VAL> Epoch {}: loss: {}.\".format(epoch, loss_sum/val_batches))\n",
        "\n",
        "        #####################\n",
        "        # Save model parameters.\n",
        "        #####################\n",
        "        if epoch % 20 == 0:\n",
        "          checkpoint_path = os.path.join(model_path, 'model.ckpt')\n",
        "          saver.save(sess, checkpoint_path, global_step=epoch)\n",
        "\n",
        "\n",
        "def main():\n",
        "  train_dataset = dataset.Dataset(base_path=\"/content/ShapeNetCore_im2avatar\", \n",
        "                                  cat_id=cat_id, \n",
        "                                  data_list_path=\"/content/drive/My Drive/im2avatar/data_list\")\n",
        "\n",
        "  train(train_dataset)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8hNDU7Qh6XG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}